{"version":"NotebookV1","origId":779539500130211,"name":"_utility-methods","language":"python","commands":[{"version":"CommandV1","origId":779539500130212,"guid":"e342d744-13d9-4972-801c-87a6e9f1f06d","subtype":"command","commandType":"auto","position":2.0,"command":"%pip install \\\ngit+https://github.com/databricks-academy/dbacademy-gems \\\ngit+https://github.com/databricks-academy/dbacademy-rest \\\n--quiet --disable-pip-version-check","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"9ec93c5a-f5e9-44ec-baf3-eeaf72c73b5e"},{"version":"CommandV1","origId":779539500130213,"guid":"ce1d873c-232c-4bc4-8563-9d24a4f65b25","subtype":"command","commandType":"auto","position":3.0,"command":"class Paths():\n    def __init__(self, working_dir, clean_lesson):\n        self.working_dir = working_dir\n\n        if clean_lesson: self.user_db = f\"{working_dir}/{clean_lesson}.db\"\n        else:            self.user_db = f\"{working_dir}/database.db\"\n            \n    def exists(self, path):\n        try: return len(dbutils.fs.ls(path)) >= 0\n        except Exception:return False\n\n    def print(self, padding=\"  \"):\n        max_key_len = 0\n        for key in self.__dict__: max_key_len = len(key) if len(key) > max_key_len else max_key_len\n        for key in self.__dict__:\n            label = f\"{padding}DA.paths.{key}:\"\n            print(label.ljust(max_key_len+13) + DA.paths.__dict__[key])\n\nclass DBAcademyHelper():\n    def __init__(self, lesson=None):\n        import re, time\n\n        self.start = int(time.time())\n        \n        self.course_name = \"dewd\"\n        self.lesson = lesson.lower()\n        self.data_source_uri = \"wasbs://courseware@dbacademy.blob.core.windows.net/data-engineering-with-databricks/v02\"\n\n        # Define username\n        self.username = spark.sql(\"SELECT current_user()\").first()[0]\n        self.clean_username = re.sub(\"[^a-zA-Z0-9]\", \"_\", self.username)\n\n        self.db_name_prefix = f\"dbacademy_{self.clean_username}_{self.course_name}\"\n        self.source_db_name = None\n\n        self.working_dir_prefix = f\"dbfs:/user/{self.username}/dbacademy/{self.course_name}\"\n        \n        if self.lesson:\n            clean_lesson = re.sub(\"[^a-zA-Z0-9]\", \"_\", self.lesson)\n            working_dir = f\"{self.working_dir_prefix}/{self.lesson}\"\n            self.paths = Paths(working_dir, clean_lesson)\n            self.hidden = Paths(working_dir, clean_lesson)\n            self.db_name = f\"{self.db_name_prefix}_{clean_lesson}\"\n        else:\n            working_dir = self.working_dir_prefix\n            self.paths = Paths(working_dir, None)\n            self.hidden = Paths(working_dir, None)\n            self.db_name = self.db_name_prefix\n\n    def init(self, create_db=True):\n        spark.catalog.clearCache()\n        self.create_db = create_db\n        \n        if create_db:\n            print(f\"\\nCreating the database \\\"{self.db_name}\\\"\")\n            spark.sql(f\"CREATE DATABASE IF NOT EXISTS {self.db_name} LOCATION '{self.paths.user_db}'\")\n            spark.sql(f\"USE {self.db_name}\")\n\n    def cleanup(self):\n        for stream in spark.streams.active:\n            print(f\"Stopping the stream \\\"{stream.name}\\\"\")\n            stream.stop()\n            try: stream.awaitTermination()\n            except: pass # Bury any exceptions\n\n        if spark.sql(f\"SHOW DATABASES\").filter(f\"databaseName == '{self.db_name}'\").count() == 1:\n            print(f\"Dropping the database \\\"{self.db_name}\\\"\")\n            spark.sql(f\"DROP DATABASE {self.db_name} CASCADE\")\n            \n        if self.paths.exists(self.paths.working_dir):\n            print(f\"Removing the working directory \\\"{self.paths.working_dir}\\\"\")\n            dbutils.fs.rm(self.paths.working_dir, True)\n\n    def conclude_setup(self):\n        import time\n\n        spark.conf.set(\"da.username\", self.username)\n        spark.conf.set(\"da.db_name\", self.db_name)\n        for key in self.paths.__dict__:\n            spark.conf.set(f\"da.paths.{key.lower()}\", self.paths.__dict__[key])\n\n        print(\"\\nPredefined Paths:\")\n        DA.paths.print()\n\n        if self.source_db_name:\n            print(f\"\\nPredefined tables in {self.source_db_name}:\")\n            tables = spark.sql(f\"SHOW TABLES IN {self.source_db_name}\").filter(\"isTemporary == false\").select(\"tableName\").collect()\n            if len(tables) == 0: print(\"  -none-\")\n            for row in tables: print(f\"  {row[0]}\")\n\n        if self.create_db:\n            print(f\"\\nPredefined tables in {self.db_name}:\")\n            tables = spark.sql(f\"SHOW TABLES IN {self.db_name}\").filter(\"isTemporary == false\").select(\"tableName\").collect()\n            if len(tables) == 0: print(\"  -none-\")\n            for row in tables: print(f\"  {row[0]}\")\n                \n        print(f\"\\nSetup completed in {int(time.time())-self.start} seconds\")\n        \n    def block_until_stream_is_ready(self, query, min_batches=2):\n        import time\n        while len(query.recentProgress) < min_batches:\n            time.sleep(5) # Give it a couple of seconds\n\n        print(f\"The stream has processed {len(query.recentProgress)} batchs\")\n        \ndbutils.widgets.text(\"lesson\", \"missing\")\nlesson = dbutils.widgets.get(\"lesson\")\nif lesson == \"none\": lesson = None\nassert lesson != \"missing\", f\"The lesson must be passed to the DBAcademyHelper\"\n\nDA = DBAcademyHelper(lesson)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"a77a71dc-34c0-4fbb-a342-fbbde4a00762"},{"version":"CommandV1","origId":779539500130214,"guid":"a6d79722-6e41-413d-bb98-d99f2659381f","subtype":"command","commandType":"auto","position":4.0,"command":"def install_source_dataset(source_uri, reinstall, subdir):\n    target_dir = f\"{DA.working_dir_prefix}/source/{subdir}\"\n\n#     if reinstall and DA.paths.exists(target_dir):\n#         print(f\"Removing existing dataset at {target_dir}\")\n#         dbutils.fs.rm(target_dir, True)\n    \n    if DA.paths.exists(target_dir):\n        print(f\"Skipping install to \\\"{target_dir}\\\", dataset already exists\")\n    else:\n        print(f\"Installing datasets to \\\"{target_dir}\\\"\")\n        dbutils.fs.cp(source_uri, target_dir, True)\n        \n    return target_dir","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"c625f230-caeb-4fe1-b5b0-b07695e2ba6a"},{"version":"CommandV1","origId":779539500130215,"guid":"d5c8dcd1-ca5a-4c9b-a4fe-e71100c76ab4","subtype":"command","commandType":"auto","position":5.0,"command":"def install_dtavod_datasets(reinstall):\n    source_uri = \"wasbs://courseware@dbacademy.blob.core.windows.net/databases_tables_and_views_on_databricks/v02\"\n    DA.paths.datasets = install_source_dataset(source_uri, reinstall, \"dtavod\")\n\n    copy_source_dataset(f\"{DA.paths.datasets}/flights/departuredelays.csv\", \n                        f\"{DA.paths.working_dir}/flight_delays\",\n                        format=\"csv\", name=\"flight_delays\")","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"c27b2f24-ca86-4aee-b062-87489bc0dc38"},{"version":"CommandV1","origId":779539500130216,"guid":"a3921e0f-fb02-40d7-a650-9f42c2c4e8d3","subtype":"command","commandType":"auto","position":6.0,"command":"def install_eltwss_datasets(reinstall):\n    source_uri = \"wasbs://courseware@dbacademy.blob.core.windows.net/elt-with-spark-sql/v02/small-datasets\"\n    DA.paths.datasets = install_source_dataset(source_uri, reinstall, \"eltwss\")","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"39a76b39-01eb-4282-aa8f-b6ed46586135"},{"version":"CommandV1","origId":779539500130217,"guid":"b79f21ad-da43-4839-8986-d6b51791f84e","subtype":"command","commandType":"auto","position":7.0,"command":"def clone_source_table(table_name, source_path, source_name=None):\n    import time\n    start = int(time.time())\n\n    source_name = table_name if source_name is None else source_name\n    print(f\"Cloning the {table_name} table from {source_path}/{source_name}\", end=\"...\")\n    \n    spark.sql(f\"\"\"\n        CREATE OR REPLACE TABLE {table_name}\n        SHALLOW CLONE delta.`{source_path}/{source_name}`\n        \"\"\")\n\n    total = spark.read.table(table_name).count()\n    print(f\"({int(time.time())-start} seconds / {total:,} records)\")\n    \ndef load_eltwss_tables():\n    clone_source_table(\"events\", f\"{DA.paths.datasets}/delta\")\n    clone_source_table(\"sales\", f\"{DA.paths.datasets}/delta\")\n    clone_source_table(\"users\", f\"{DA.paths.datasets}/delta\")\n    clone_source_table(\"transactions\", f\"{DA.paths.datasets}/delta\")    ","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"3c9a2e94-8044-48f2-9bbb-25de68d86c72"},{"version":"CommandV1","origId":779539500130218,"guid":"111267e6-6dac-4b95-aa36-00f0946921e9","subtype":"command","commandType":"auto","position":8.0,"command":"def copy_source_dataset(src_path, dst_path, format, name):\n    import time\n    start = int(time.time())\n    print(f\"Creating the {name} dataset\", end=\"...\")\n    \n    dbutils.fs.cp(src_path, dst_path, True)\n\n    total = spark.read.format(format).load(dst_path).count()\n    print(f\"({int(time.time())-start} seconds / {total:,} records)\")\n    \ndef load_eltwss_external_tables():\n    copy_source_dataset(f\"{DA.paths.datasets}/raw/sales-csv\", \n                        f\"{DA.paths.working_dir}/sales-csv\", \"csv\", \"sales-csv\")\n\n    import time\n    start = int(time.time())\n    print(f\"Creating the users table\", end=\"...\")\n\n    # REFACTORING - Making lesson-specific copy\n    dbutils.fs.cp(f\"{DA.paths.datasets}/raw/users-historical\", \n                  f\"{DA.paths.working_dir}/users-historical\", True)\n\n    # https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html\n    (spark.read\n          .format(\"parquet\")\n          .load(f\"{DA.paths.working_dir}/users-historical\")\n          .repartition(1)\n          .write\n          .format(\"org.apache.spark.sql.jdbc\")\n          .option(\"url\", f\"jdbc:sqlite:/{DA.username}_ecommerce.db\")\n          .option(\"dbtable\", \"users\") # The table name in sqllight\n          .mode(\"overwrite\")\n          .save())\n\n    total = spark.read.parquet(f\"{DA.paths.working_dir}/users-historical\").count()\n    print(f\"({int(time.time())-start} seconds / {total:,} records)\")","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"ef6c5957-fe1a-4f0a-8847-4674fe2ce6c6"},{"version":"CommandV1","origId":779539500130219,"guid":"ea70418c-a2aa-47f1-97e0-eb25cf3060d8","subtype":"command","commandType":"auto","position":9.0,"command":"# lesson: Writing delta \ndef create_eltwss_users_update():\n    import time\n    start = int(time.time())\n    print(f\"Creating the users_dirty table\", end=\"...\")\n\n    # REFACTORING - Making lesson-specific copy\n    dbutils.fs.cp(f\"{DA.paths.datasets}/raw/users-30m\", \n                  f\"{DA.paths.working_dir}/users-30m\", True)\n    \n    spark.sql(f\"\"\"\n        CREATE OR REPLACE TABLE users_dirty AS\n        SELECT *, current_timestamp() updated \n        FROM parquet.`{DA.paths.working_dir}/users-30m`\n    \"\"\")\n    \n    spark.sql(\"INSERT INTO users_dirty VALUES (NULL, NULL, NULL, NULL), (NULL, NULL, NULL, NULL), (NULL, NULL, NULL, NULL)\")\n    \n    total = spark.read.table(\"users_dirty\").count()\n    print(f\"({int(time.time())-start} seconds / {total:,} records)\")","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"36705225-2d30-4cbf-b5ba-eba5ba7078cd"},{"version":"CommandV1","origId":779539500130220,"guid":"dbc1e55b-d5ac-4efb-ab6e-0127070f5746","subtype":"command","commandType":"auto","position":10.0,"command":"class DltDataFactory:\n    def __init__(self):\n        self.source = f\"/mnt/training/healthcare/tracker/streaming\"\n        self.userdir = f\"{DA.paths.working_dir}/source/tracker\"\n        try:\n            self.curr_mo = 1 + int(max([x[1].split(\".\")[0] for x in dbutils.fs.ls(self.userdir)]))\n        except:\n            self.curr_mo = 1\n    \n    def load(self, continuous=False):\n        if self.curr_mo > 12:\n            print(\"Data source exhausted\\n\")\n        elif continuous == True:\n            while self.curr_mo <= 12:\n                curr_file = f\"{self.curr_mo:02}.json\"\n                target_dir = f\"{self.userdir}/{curr_file}\"\n                print(f\"Loading the file {curr_file} to the {target_dir}\")\n                dbutils.fs.cp(f\"{self.source}/{curr_file}\", target_dir)\n                self.curr_mo += 1\n        else:\n            curr_file = f\"{str(self.curr_mo).zfill(2)}.json\"\n            target_dir = f\"{self.userdir}/{curr_file}\"\n            print(f\"Loading the file {curr_file} to the {target_dir}\")\n\n            dbutils.fs.cp(f\"{self.source}/{curr_file}\", target_dir)\n            self.curr_mo += 1\n","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"metadata":{}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"7065ff98-7993-45db-83e7-20cb7e29e7fe"}],"dashboards":[],"guid":"b6492967-455f-4763-97d8-96fe82952dae","globalVars":{},"iPythonMetadata":null,"inputWidgets":{},"notebookMetadata":{"pythonIndentUnit":2},"reposExportFormat":"SOURCE"}